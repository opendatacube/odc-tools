{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to use Geometric Median for time dimension reduction. Geomedian computation is quite expensive in terms of memory, data bandwidth and cpu usage. We use Dask to perform data loading and computation in parallel across many threads to speed things up. In this notebook local Dask cluster is used, but the same approach should work using a larger, distributed Dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install missing requirements\n",
    "\n",
    "```\n",
    "pip install --user --extra-index-url=\"https://packages.dea.ga.gov.au\" hdstats\n",
    "pip install --user --extra-index-url=\"https://packages.dea.ga.gov.au\" odc-algo\n",
    "```\n",
    "\n",
    "Verify install worked by importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdstats\n",
    "import odc.algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup local dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datacube.utils.rio import configure_s3_access\n",
    "from datacube.utils.dask import start_local_dask\n",
    "import os\n",
    "import dask\n",
    "from dask.utils import parse_bytes\n",
    "\n",
    "# configure dashboard link to go over proxy\n",
    "dask.config.set({\"distributed.dashboard.link\":\n",
    "                 os.environ.get('JUPYTERHUB_SERVICE_PREFIX', '/')+\"proxy/{port}/status\"});\n",
    "\n",
    "# Figure out how much memory/cpu we really have (those are set by jupyterhub)\n",
    "mem_limit = int(os.environ.get('MEM_LIMIT', '0'))\n",
    "cpu_limit = float(os.environ.get('CPU_LIMIT', '0'))\n",
    "cpu_limit = int(cpu_limit) if cpu_limit > 0 else 4\n",
    "mem_limit = mem_limit if mem_limit > 0 else parse_bytes('8Gb')\n",
    "\n",
    "# leave 4Gb for notebook itself\n",
    "mem_limit -= parse_bytes('4Gb')\n",
    "\n",
    "# close previous client if any, so that one can re-run this cell without issues\n",
    "client = locals().get('client', None)\n",
    "if client is not None:\n",
    "    client.close()\n",
    "    del client\n",
    "    \n",
    "client = start_local_dask(n_workers=1,\n",
    "                          threads_per_worker=cpu_limit, \n",
    "                          memory_limit=mem_limit)\n",
    "display(client)\n",
    "\n",
    "# Configure GDAL for s3 access \n",
    "configure_s3_access(aws_unsigned=True,  # works only when reading public resources\n",
    "                    client=client);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Datacube and data source\n",
    "\n",
    "In this notebook we are using `ls8_ard` and will be computing Geomedian for one Landsat scene (96, 74) using all available observations for the year 2016. To limit computation and memory this example uses only three optical bands (red, green, blue) and we limit computation to a 2K by 2K block of pixels roughly in the middle of the scene.\n",
    "\n",
    "Cell bellow finds all  the datasets of interest. These all should be in the same projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datacube import Datacube\n",
    "from odc.algo import fmask_to_bool, to_f32, from_float, xr_geomedian\n",
    "\n",
    "dc = Datacube()\n",
    "\n",
    "product = 'ls8_ard'\n",
    "region_code, year = '96074', 2016\n",
    "\n",
    "dss = dc.find_datasets(product=product, \n",
    "                       region_code=region_code, \n",
    "                       time=str(year))\n",
    "len(dss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do native load (lazy version with Dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bands = ['red', 'green', 'blue']\n",
    "mask_bands = ['fmask']\n",
    "\n",
    "xx = dc.load(product=dss[0].type.name,\n",
    "             output_crs=dss[0].crs,\n",
    "             resolution=(-30, 30),\n",
    "             align=(15, 15),\n",
    "             measurements=data_bands + mask_bands,\n",
    "             group_by='solar_day',\n",
    "             datasets=dss, \n",
    "             dask_chunks=dict(\n",
    "                 x=1000, \n",
    "                 y=1000)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a 2k by 2k subsection, to speed up testing\n",
    "xx = xx.isel(x=np.s_[4000:6000], y=np.s_[4000:6000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Geomedian on data_bands\n",
    "1. Convert fmask to boolean: `True` - use, `False` - do not use\n",
    "2. Apply masking in native dtype for data bands only\n",
    "3. Convert to `float32` with scaling\n",
    "4. Reduce time dimension with geometric median\n",
    "5. Convert back to native dtype with scaling\n",
    "\n",
    "All steps are dask operations, so no actual computation is done until `.compute()` is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale, offset = (1/10_000, 0)  # differs per product, aim for 0-1 values in float32\n",
    "\n",
    "no_cloud = fmask_to_bool(xx.fmask, ('valid', 'snow', 'water'))\n",
    "\n",
    "xx_data = xx[data_bands]\n",
    "xx_clean = odc.algo.keep_good_only(xx_data, where=no_cloud)\n",
    "xx_clean = to_f32(xx_clean, scale=scale, offset=offset)\n",
    "yy = xr_geomedian(xx_clean, \n",
    "                  num_threads=1,  # disable internal threading, dask will run several concurrently\n",
    "                  eps=0.2*scale,  # 1/5 pixel value resolution\n",
    "                  nocheck=True)   # disable some checks inside geomedian library that use too much ram\n",
    "\n",
    "yy = from_float(yy, \n",
    "                dtype='int16', \n",
    "                nodata=-999, \n",
    "                scale=1/scale, \n",
    "                offset=-offset/scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can run the computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "yy = yy.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to RGBA and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odc.ui import to_rgba, to_png_data\n",
    "from IPython.display import Image\n",
    "\n",
    "rgba = to_rgba(yy, clamp=3000)\n",
    "Image(data=to_png_data(rgba.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
